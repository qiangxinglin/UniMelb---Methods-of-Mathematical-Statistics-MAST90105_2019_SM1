---
title: "Assignment 3"
author: "Vinh Nguyen (ID 1029531)"
output: html_notebook
---


```{r}
# Preliminaries
rm(list=ls())
```
1. Let X1, . . . , Xn and Y1, . . . , Ym are repeated measurements of the nitrogen dioxide
obtained by device A and device B, respectively. Measurement errors are different for
these two devices and we assume that the measurements X1, . . . , Xn, Y1, . . . , Ym are
all mutually independent and Xi ∼ N(µ, σ2
), Yj ∼ N(µ, 2σ
2
) for i = 1, . . . , n and
j = 1, . . . , m. We want to estimate unknown parameters µ and σ
2 using the likelihood
approach.
(a) Write down the joint pdf of X1, . . . , Xn, Y1, . . . , Ym and simplify it.
$$
\begin{align}
f_{X_1, X_2, \dots, X_n, Y_1, Y_2, \dots, Y_m}(x_1, x_2, \dots, x_n, y_1, y_2, \dots, y_m) &= f_{X_1}(x_1)\dots f_{X_n}(x_n) f_{Y_1}(y_1)\dots f_{Y_m}(y_m) \text{ (independence)} \\
&= \prod_{i=1}^{n}f_{X_i}(x_i) \prod_{i=1}^{m}f_{Y_i}(y_i) \\
&= \prod_{i=1}^{n} \frac{1}{\sqrt{2\pi}\sigma} e^{-\frac{1}{2}\frac{(x_i - \mu)^{2}}{\sigma^{2}}} \prod_{i=1}^{m} \frac{1}{\sqrt{2\pi}\sqrt{2}\sigma} e^{-\frac{1}{2}\frac{(y_i - \mu)^{2}}{2\sigma^{2}}} \\
&= \bigg(\frac{1}{\sqrt{2\pi}\sigma}\bigg)^n e^{\sum_{i=1}^{n} -\frac{1}{2}\frac{(x_i - \mu)^{2}}{\sigma^{2}}}  \bigg(\frac{1}{\sqrt{2\pi}\sqrt{2}\sigma}\bigg)^m e^{\sum_{i=1}^{m} -\frac{1}{2}\frac{(y_i - \mu)^{2}}{2\sigma^{2}}} \\
&= \frac{1}{\Big(\sqrt{2}\Big)^m \Big(\sqrt{2\pi}\Big)^{n+m} \sigma^{n+m}} 
e^{\sum_{i=1}^{n} -\frac{1}{2}\frac{(x_i - \mu)^{2}}{\sigma^{2}} + \sum_{i=1}^{m} -\frac{1}{2}\frac{(y_i - \mu)^{2}}{2\sigma^{2}}} \\
\end{align}
$$

(b) Find the maximum likelihood estimators for µ and σ
2
(You are not required to
demonstrate that the stationary points are maxima).  
Likelihood function:
$$
\begin{align}
L(\mu, \sigma) = f_{X_i, Y_i}(x_i, y_i) &= \frac{1}{\Big(\sqrt{2}\Big)^m \Big(\sqrt{2\pi}\Big)^{n+m} \sigma^{n+m}}
e^{\sum_{i=1}^{n} -\frac{1}{2}\frac{(x_i - \mu)^{2}}{\sigma^{2}} + \sum_{i=1}^{m} -\frac{1}{2}\frac{(y_i - \mu)^{2}}{2\sigma^{2}}} \\
&= \frac{1}{\Big(\sqrt{2}\Big)^m \Big(\sqrt{2\pi}\Big)^{n+m}} \sigma^{-(n+m)} e^{\sum_{i=1}^{n} -\frac{1}{2}\frac{(x_i - \mu)^{2}}{\sigma^{2}} + \sum_{i=1}^{m} -\frac{1}{2}\frac{(y_i - \mu)^{2}}{2\sigma^{2}}} \\
ln(L(\mu, \sigma)) &= C - (n+m)ln\sigma + \sum_{i=1}^{n} -\frac{1}{2}\frac{(x_i - \mu)^{2}}{\sigma^{2}} + \sum_{i=1}^{m} -\frac{1}{2}\frac{(y_i - \mu)^{2}}{2\sigma^{2}} \quad \bigg(C = ln\frac{1}{\Big(\sqrt{2}\Big)^m \Big(\sqrt{2\pi}\Big)^{n+m}}\bigg) \\
\end{align}
$$
Maximum likelihood estimator for µ:
$$
\begin{align}
\frac{\partial ln(L(\mu, \sigma))}{\partial \mu} &= \sum_{i=1}^{n} \frac{x_i - \mu}{\sigma^{2}} + \sum_{i=1}^{m} \frac{y_i - \mu}{2\sigma^{2}} \\
&= \frac{1}{\sigma^2}\bigg(\sum_{i=1}^{n} x_i - n\mu\bigg) + \frac{1}{2\sigma^2} \bigg(\sum_{i=1}^{m} y_i - m\mu\bigg) \\
\end{align}
$$
$$
\begin{align}
\frac{\partial ln(L(\mu, \sigma))}{\partial \mu} &\stackrel{\text{set}}{=} 0 \\
\frac{1}{\sigma^2}\bigg(\sum_{i=1}^{n} x_i - n\mu\bigg) + \frac{1}{2\sigma^2} \bigg(\sum_{i=1}^{m} y_i - m\mu\bigg) &= 0 \\
2\sum_{i=1}^{n} x_i + \sum_{i=1}^{m} y_i - (2n + m)\mu &= 0 \\
\mu &= \frac{2\sum_{i=1}^{n} x_i + \sum_{i=1}^{m} y_i}{2n + m} \\
\end{align}
$$
$$
\begin{align}
\hat{\mu} = \frac{2\sum_{i=1}^{n} X_i + \sum_{i=1}^{m} Y_i}{2n + m} \\
\end{align}
$$
Maximum likelihood estimator for σ^2:
$$
\begin{align}
\frac{\partial ln(L(\mu, \sigma))}{\partial (\sigma^2)} &= -(n+m)\frac{1}{\sigma} \frac{1}{2 \sigma} - \frac{1}{\sigma^4} \sum_{i=1}^{n} -\frac{1}{2}(x_i - \mu)^{2} - \frac{1}{\sigma^4} \sum_{i=1}^{m} -\frac{1}{2}\frac{(y_i - \mu)^{2}}{2} \\
&= -(n+m)\frac{1}{2\sigma^2} + \frac{1}{2\sigma^4} \sum_{i=1}^{n}(x_i - \mu)^{2} + \frac{1}{4\sigma^4} \sum_{i=1}^{m} (y_i - \mu)^{2} \\
\end{align}
$$
$$
\begin{align}
\frac{\partial ln(L(\mu, \sigma))}{\partial (\sigma^2)} \stackrel{\text{set}}{=} 0 \\
-(n+m)\frac{1}{2\sigma^2} + \frac{1}{2\sigma^4} \sum_{i=1}^{n}(x_i - \mu)^{2} + \frac{1}{4\sigma^4} \sum_{i=1}^{m} (y_i - \mu)^{2} = 0 \\
\sigma^2 = \frac{2 \sum_{i=1}^{n} (x_i - \mu)^{2} + \sum_{i=1}^{m} (y_i - \mu)^{2}}{2(n+m)} \quad \bigg(\mu = \frac{2\sum_{i=1}^{n} x_i + \sum_{i=1}^{m} y_i}{2n + m}\bigg) \\
\end{align}
$$
$$
\begin{align}
\hat{\sigma}^2 = \frac{2 \sum_{i=1}^{n} (X_i - \hat{\mu})^{2} + \sum_{i=1}^{m} (Y_i - \hat{\mu})^{2}}{2(n+m)} \quad \bigg(\hat{\mu} = \frac{2\sum_{i=1}^{n} X_i + \sum_{i=1}^{m} Y_i}{2n + m}\bigg) \\
\end{align}
$$

(c) Let n = 10 and m = 8. Use the following data to find estimates of µ and σ
2
:
X: 3.39 2.19 2.18 1.57 1.30 3.52 2.41 2.00 2.87 3.17
Y: 1.01 1.97 3.51 1.53 1.88 2.34 1.14 1.29
```{r}
x <- c(3.39, 2.19, 2.18, 1.57, 1.30, 3.52, 2.41, 2.00, 2.87, 3.17)
y <- c(1.01, 1.97, 3.51, 1.53, 1.88, 2.34, 1.14, 1.29)

"mu estimate"
mu <- (2*sum(x) + sum(y)) / (2*length(x) + length(y))
mu

"variance estimate"
sigma_sq <- (2*sum((x-mu)^2) + sum((y-mu)^2)) / (2*(length(x)+length(y)))
sigma_sq
```

2. Let X1, . . . , Xn be a random sample from a continuous distribution with density
f(x; a, b) = (
ax2 + bx +
1
2 −
a
3
, −1 < x < 1,
0, otherwise.
(a) Find the method of moments estimators for unknown parameters a and b.  
$$
\begin{align}
E[X] &= \int_{-\infty}^{\infty} xf(x; a,b)dx \\
&= \int_{-1}^{1} x \bigg(ax^2 + bx + \frac{1}{2} - \frac{a}{3}\bigg) dx \\
&= \int_{-1}^{1} \bigg(ax^3 + bx^2 + \bigg(\frac{1}{2} - \frac{a}{3}\bigg)x\bigg)dx \\
&= \bigg[\frac{1}{4}ax^4 + \frac{1}{3}bx^3 + \frac{1}{2}\bigg(\frac{1}{2} - \frac{a}{3}\bigg)x^2\bigg]\Bigg|_{-1}^{1} \\
&= \bigg[\frac{1}{4}a + \frac{1}{3}b + \frac{1}{2}\bigg(\frac{1}{2} - \frac{a}{3}\bigg)\bigg] - \bigg[\frac{1}{4}a - \frac{1}{3}b + \frac{1}{2}\bigg(\frac{1}{2} - \frac{a}{3}\bigg)\bigg] \\
&= \frac{2}{3}b \\
\end{align}
$$
$$
\begin{align}
E[X^2] &= \int_{-\infty}^{\infty} x^2 f(x; a,b)dx \\
&= \int_{-1}^{1} x^2 \bigg(ax^2 + bx + \frac{1}{2} - \frac{a}{3}\bigg) dx \\
&= \int_{-1}^{1} \bigg(ax^4 + bx^3 + \bigg(\frac{1}{2} - \frac{a}{3}\bigg)x^2\bigg)dx \\
&= \bigg[\frac{1}{5}ax^5 + \frac{1}{4}bx^4 + \frac{1}{3}\bigg(\frac{1}{2} - \frac{a}{3}\bigg)x^3\bigg]\Bigg|_{-1}^{1} \\
&= \bigg[\frac{1}{5}a + \frac{1}{4}b + \frac{1}{3}\bigg(\frac{1}{2} - \frac{a}{3}\bigg)\bigg] - \bigg[-\frac{1}{5}a + \frac{1}{4}b - \frac{1}{3}\bigg(\frac{1}{2} - \frac{a}{3}\bigg)\bigg] \\
&= \frac{8}{45}a + \frac{1}{3} \\
\end{align}
$$
**Method of moments estimators**  
$$
\begin{align}
& \left\{
\begin{array}{cl}
E(X) & \stackrel{set}{=} \overline{X} \\
E(X^2) & \stackrel{set}{=} \overline{X^2} \\
\end{array}
\right. \\
& \left\{
\begin{array}{cl}
\frac{2}{3}b &= \overline{X} \\
\frac{8}{45}a + \frac{1}{3} &= \overline{X^2} \\
\end{array}
\right. \\
& \left\{
\begin{array}{cl}
\hat{b} &= \frac{3}{2} \overline{X} \\
\hat{a} &= \frac{45}{8} \bigg(\overline{X^2} - \frac{1}{3}\bigg) \\
\end{array}
\right. \\
\end{align}
$$

(b) Are the method of moments estimators of a and b unbiased?  
$$
\begin{align}
E(\hat{b}) &= \frac{3}{2} E(\overline{X}) \\
&= \frac{3}{2} E(X_1) \quad (\overline{X} \text{ is an unbiased estimator for } \mu_{X_1} = E(X_1)) \\
&= \frac{3}{2} \frac{2}{3}b \\
&= b \\
\Rightarrow \hat{b} & \text{ is unbiased}
\end{align} \\
$$
$$
\begin{align}
E(\hat{a}) &= \frac{45}{8} E\bigg(\overline{X^2} - \frac{1}{3}\bigg) \\
&= \frac{45}{8} \bigg(E\big(\overline{X^2}\big) - \frac{1}{3}\bigg) \\
&= \frac{45}{8} \bigg(E\big(X_1^2\big) - \frac{1}{3}\bigg) \quad (\overline{X^2} \text{ is an unbiased estimator for } \mu_{X_1^2}=E(X_1^2)) \\
&= \frac{45}{8} \bigg(\frac{8}{45}a + \frac{1}{3} - \frac{1}{3}\bigg) \\
&= a \\
\Rightarrow \hat{a} & \text{ is unbiased}
\end{align}
$$

(c) Let n = 10. Use the following data to find method of moments estimates of a
and b:
-0.77 0.33 -0.61 -0.27 -0.13 0.21 0.09 -0.46 0.85 0.43  
```{r}
x <- c(-0.77, 0.33, -0.61, -0.27, -0.13, 0.21, 0.09, -0.46, 0.85, 0.43)
"a estimate"
a.hat <- 45/8 * (mean(x^2) - 1/3)
a.hat
"b estimate"
b.hat <- 3/2 * mean(x)
b.hat
```

3. Let X1, . . . , Xn be a random sample from a continuous distribution with density:
f(x; c) =



c, −1 < 0 < x,
1−c
3
, 0 ≤ x < 3,
0, otherwise.  
(a) Find the method of moments estimator for c. Is it an unbiased estimator?  
$$
\begin{align}
E[X] &= \int_{-\infty}^{\infty} xf(x; c)dx \\
&= \int_{-1}^{0} xc dx + \int_{0}^{3} x\frac{1-c}{3} dx \\
&= \frac{1}{2}cx^2\Big|_{-1}^{0} + \frac{1}{6} (1-c) x^2 \Big|_{0}^{3} \\
&= \frac{1}{2}c(-1) + \frac{1}{6} (1-c) 9 \\
&= -2c + \frac{3}{2} \\
E[X] & \stackrel{set}{=} \overline{X} \\
-2c + \frac{3}{2} &= \overline{X} \\
\hat{c} &= -\frac{1}{2} \overline{X} + \frac{3}{4} \\
\end{align}
$$
$$
\begin{align}
E(\hat{c}) &= E\bigg(-\frac{1}{2} \overline{X} + \frac{3}{4}\bigg) \\
&= -\frac{1}{2} E(\overline{X}) + \frac{3}{4} \\
&= -\frac{1}{2} E(X) + \frac{3}{4} \quad (\overline{X} \text{ is an unbiased estimator for } parameter E(X)) \\
&= -\frac{1}{2} \bigg(-2c + \frac{3}{2}\bigg) + \frac{3}{4} \\
&= c \\
\Rightarrow \hat{c} & \text{ unbiased} \\
\end{align}
$$

(b) Find the variance of the method of moments estimator as a function of c.  
$$
\begin{align}
var(\hat{c}) &= var\bigg(-\frac{1}{2} \overline{X} + \frac{3}{4}\bigg) \\
&= \frac{1}{4} var(\overline{X}) \\
&= \frac{1}{4} \frac{1}{n} var(X_1) \quad (X_1, \dots , X_n \text{ i.i.d}) \\
\end{align}
$$
Calculate $var(X_1)$
$$
\begin{align}
var(X_1) &= E[X_1^2] - (E[X_1])^2 \\
E[X_1^2] &= \int_{-\infty}^{\infty} x^2 f(x; c) dx \\
&= \int_{-1}^{0} x^2 c dx + \int_{0}^{3} x^2 \frac{1-c}{3} dx \\
&= \frac{1}{3}cx^3\Big|_{-1}^{0} + \frac{1}{9} (1-c) x^3 \Big|_{0}^{3} \\
&= -\frac{8}{3}c + 3 \\
var(X_1) &= -\frac{8}{3}c + 3 - \bigg(-2c + \frac{3}{2}\bigg)^2 \quad (\text{plugging } E[X_1^2], E[X_1] \text{ in}) \\
&= -\frac{8}{3}c + 3 - \bigg(4c^2 - 6c + \frac{9}{4}\bigg) \\
&= -4c^2 + \frac{10}{3}c + \frac{3}{4} \\
\end{align}
$$
**Variance of the method of moments estimator**  
$$
\begin{align}
var(\hat{c}) &= \frac{1}{4n} \bigg(-4c^2 + \frac{10}{3}c + \frac{3}{4}\bigg) \\
\end{align}
$$

(c) Let M be the number of variables Xi such that −1 < Xi < 0 and n − M be the
number of variables Xi such that 0 ≤ Xi < 3. Write down the likelihood function
for these data and find the maximum likelihood estimator of c.  
Let $Y_i$: variable ${X_i}$ such that $−1 < X_i < 0$ $\Rightarrow f_{Y_i}(y) = c \ ,\ −1 < y < 0$
$Z_i$: variable ${X_i}$ such that $0 ≤ X_i < 3$ $\Rightarrow f_{Z_i}(z) = \frac{1-c}{3} \ ,\ 0 ≤ z < 3$
Likelihood function
$$
\begin{align}
L(c) &= \prod_{i=1}^{n} f_{X_{i}}(x_i; c) \\
&= \prod_{i=1}^{M} f_{Y_{i}}(y_i) \prod_{i=1}^{n-M} f_{Z_{i}}(z_i) \quad (−1 < y_i < 0,\ 0 ≤ z_i < 3) \\
&= c^M \bigg(\frac{1-c}{3}\bigg)^{n-M} \\
&= \frac{1}{3^{n-M}} c^M (1-c)^{n-M} \\
lnL(c) &= ln\bigg(\frac{1}{3^{n-M}}\bigg) + Mlnc + (n-M)ln(1-c) \\
\end{align}
$$
Maximum likelihood estimator  
$$
\begin{align}
\frac{\partial lnL(c)}{\partial c} &= M\frac{1}{c} - (n-M)\frac{1}{1-c} \stackrel{set}{=} 0 \\
M\frac{1}{c} &= (n-M)\frac{1}{1-c} \quad (c \neq 0, 1) \\
c &= \frac{1}{n} M \\
\frac{\partial^2 lnL(c)}{(\partial c)^2} &= -M\frac{1}{c^2} + (n-M)\frac{1}{(1-c)^2} < 0 \ ,\ \forall c \neq 0, 1;\ n \geq M \\
\Rightarrow \hat{c} &= \frac{1}{n} M \\
\end{align}
$$

(d) Is the maximum likelihood estimator of c unbiased? Find the variance of this
estimator as a function of c. Hint: M follows a Binomial distribution Binom(n, p)
with probability of success p = Pr(X < 0) where the pdf of X is f(x; c).  
$$
\begin{align}
E(\hat{c}) &= \frac{1}{n} E(M) \\
\end{align}
$$
Calculate $p = P(−1 < X_i < 0)$
$$
\begin{align}
p &= \int_{-1}^{0} f(x; c) dx \\
&= \int_{-1}^{0} c dx \\
&= c \\
\end{align}
$$
$M \sim Bi(n, p=c) \Rightarrow E(M) = np = nc$
$$
\begin{align}
E(\hat{c}) &= \frac{1}{n} nc \\
&= c \\
\Rightarrow \hat{c} & \text{ unbiased}
\end{align}
$$
Variance of $\hat{c}$  
$$
\begin{align}
var(\hat{c}) &= var\bigg(\frac{1}{n} M\bigg) \\
&= \frac{1}{n^2} var(M) \\
&= \frac{1}{n^2} nc(1-c) \\
&= \frac{1}{n} c(1-c) \\
\end{align}
$$

(e) Does the exact variance of the method of moments and maximum likelihood
estimators of c attain its Cramer-Rao lower bound for 0 < c < 1? Why or why
not?  
Distribution of $-\frac{\partial^2}{(\partial c)^2} ln f(X;c)$ (for 0 < c < 1)  
$$
\begin{align}
ln f(x;c) &= \left\{
\begin{array}{cl}
lnc &, −1 < x < 0 \\
ln(1-c) - ln3 &, 0 ≤ x < 3 \\
\end{array}
\right. \\
\frac{\partial}{\partial c} ln f(x;c) &= \left\{
\begin{array}{cl}
\frac{1}{c} &, −1 < x < 0 \\
-\frac{1}{1-c} &, 0 ≤ x < 3 \\
\end{array}
\right. \\
\frac{\partial^2}{(\partial c)^2} ln f(x;c) &= \left\{
\begin{array}{cl}
-\frac{1}{c^2} &, −1 < x < 0 \\
-\frac{1}{(1-c)^2} &, 0 ≤ x < 3 \\
\end{array}
\right. \\
\end{align}
$$
$$
\begin{align}
-\frac{\partial^2}{(\partial c)^2} ln f(X;c) &= \left\{
\begin{array}{cl}
\frac{1}{c^2} &, \text{ with probability } P(−1 < X < 0) = c \\
\frac{1}{(1-c)^2} &, \text{ with probability } P(0 ≤ X < 3) = 1-c \\
\end{array}
\right. \\
\end{align}
$$
Fisher Information (for 0 < c < 1)  
$$
\begin{align}
I(c) &= E\bigg(-\frac{\partial^2}{(\partial c)^2} ln f(X;c)\bigg) \\
&= \frac{1}{c^2}c + \frac{1}{(1-c)^2} (1-c) \\
&= \frac{1}{c(1-c)} \\
\end{align}
$$
Cramer-Rao Lower Bound  
$$
\begin{align}
\frac{1}{nI(c)} = \frac{1}{n} c(1-c) \\
\end{align}
$$
Variances of Method of Moments and Maximum Likelihood estimators  
$$
\begin{align}
var(\hat{c}_{MM}) &= \frac{1}{4n} \bigg(-4c^2 + \frac{10}{3}c + \frac{3}{4}\bigg) \\
&= \frac{1}{n} \bigg(-c^2 + \frac{5}{6}c + \frac{3}{16}\bigg) \\
&\neq \frac{1}{n} c(1-c) = \frac{1}{nI(c)} \\
\Rightarrow var(\hat{c}_{MM}) &> \frac{1}{nI(c)} \\
var(\hat{c}_{MLE}) &= \frac{1}{n} c(1-c) = \frac{1}{nI(c)} \\
\end{align}
$$





